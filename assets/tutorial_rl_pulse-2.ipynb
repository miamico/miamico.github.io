{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gate calibration with reinforcement learning\n\nGate-based quantum circuits are the most common representation of\nquantum computations. These provide an abstraction layer that enables\nthe development of quantum algorithms without considering the hardware\nin charge of the execution. However, every quantum platform offers a\ndifferent set of interactions and controls that define the natural\noperations that can be performed in the hardware. These are known as the\n*native gates* of the device, and they constitute the fundamental\nbuilding blocks of any quantum algorithm executed in it. Therefore, it\nis essential that such operations are performed as accurately as\npossible, which requires the careful tuning of the hardware\\'s controls.\nIn this demo, we will learn how to use reinforcement learning to find\nthe optimal control parameters to accurately execute quantum gates. We\nwill implement an experimentally-friendly protocol based on the direct\ninteraction with the hardware, following the main ideas in, which we\nillustrate using superconducting qubits as an example.\n\n![](https://blog-assets.cloud.pennylane.ai/demos/tutorial_rl_pulse/main/_assets/images/DemoOG_RLpulse.png)\n\n## Gate calibration\n\nCalibrating quantum gates consists in finding the best possible control\nparameters of the device that yield the most accurate gate execution.\nFor instance, the gates in superconducting quantum devices are performed\nby targeting the qubits with microwave pulses of the form\n\n$$\\Omega(t)\\sin(\\phi(t) + \\omega_p t)\\,,$$\n\nwhere $\\Omega(t)$ is a time-dependent amplitude, $\\phi(t)$ is a\ntime-dependent phase, and $\\omega_p$ is the pulse\\'s frequency. Hence,\nthe proper execution of any gate relies on the careful selection of\nthese parameters in combination with the pulse duration, which we\ncollectively refer to as a *pulse program*. However, each qubit in the\ndevice has distinct properties, such as the frequency and the\nconnectivity to other qubits. These differences cause the same pulse\nprograms to produce different operations for every qubit. Consequently,\nevery gate must be carefully calibrated for each individual qubit in the\nhardware. For further details about superconducting quantum computers\nand their control see [this\ndemo](https://pennylane.ai/qml/demos/oqc_pulse/).\n\nA common strategy to calibrate quantum gates involves the detailed\nmodelling of the quantum computer, enabling the gate optimization\nthrough analytical and numerical techniques. Nevertheless, developing\nsuch accurate models requires an exhaustive characterization of the\nhardware, and it can be challenging to account for all the relevant\ninteractions in practice.\n\nAn alternative promissing approach is through the direct interaction\nwith the device, refraining from deriving any explicit model of the\nsystem. Here, we frame qubit calibration as a reinforcement learning\nproblem, drawing inspiration from the experimentally-friendly method\nproposed in reference . In this setting, a reinforcement learning agent\nlearns to calibrate the gates by tuning the control parameters and\ndirectly observing the response of the qubits. Through this process, the\nagent implicitly learns an effective model of the device, as it faces\nall the experimental nuances associated with the process of executing\nthe gates, such as the effect of the most relevant noise sources. This\nmakes the resulting agent an excellent calibrator that is robust to\nthese phenomena.\n\nThe procedure that we present below is entirely agnostic to the quantum\nhardware. Among all the possibilities, we will ilustrate it using\ncoupled-transmon superconducting quantum computers simulated with\nPennyLane. This will allow us to focus on the method itself, skipping\nsome of the nuances associated with the execution on real devices, while\nensuring that the resulting code can be easily adapted to run in a\nquantum computer using the PennyLane plugins, as shown in [this\ndemo](https://pennylane.ai/qml/demos/oqc_pulse/).\n\n## Reinforcement learning basics\n\nIn the typical reinforcement learning setting, we find two main\nentities: an *agent* and an *environment*. The environment contains the\nrelevant information about the problem and defines the \"rules of the\ngame\". The main goal of the agent is to find the optimal strategy to\nperform a given task through the interaction with the environment.\n\nIn order to complete the desired task, the agent can observe the\nenvironment and perform *actions*, which can affect the environment and\nchange its *state*. This way, the interaction between them is cyclic, as\ndepicted in the figure below. Let\\'s take chess as an example. In this\ncase, the agent is a player and the environment comprises the pieces on\nthe board, the rules, and the opponent. At a given point in time, the\nagent observes the environment\\'s state, which can be the current\nlocation of all the pieces on the board. With this information, it can\nchoose to perform a certain action, such as moving a pawn forward among\nall its possible moves. Doing so affects the environment, which provides\nthe agent with the new state it is found in and a *reward*. The new\nstate is the resulting board configuration after the agent\\'s move and\nthe opponent\\'s response. The reward is a measure of how well the agent\nis performing the task given the last interaction. We will see more\nabout the reward in the following section.\n\n![](https://blog-assets.cloud.pennylane.ai/demos/tutorial_rl_pulse/main/_assets/images/sketch_rl.png)\n\nThe agent chooses its actions according to a *policy*, and **the\nultimate goal is to learn the optimal policy that maximizes the obtained\nrewards**. In general, the policy can take any form, and its nature is\nusually related to the reinforcement learning algorithm that we\nimplement to learn it. We will see how to learn the optimal policy\nlater. For now, let\\'s see how all these concepts apply to our task.\n\n## Framing qubit calibration as a reinforcement learning problem\n\nOur main objective is to accurately execute a desired quantum gate in\nour computer\\'s qubits. To do so, we need a way to find the correct\npulse program for each qubit. In reinforcement learning terms, our agent\nneeds to learn the optimal policy to obtain the pulse program for every\nqubit.\n\nIn this case, the environment is the quantum computer itself, and the\nstates encode information about the qubit\\'s evolution during the pulse\nexecution. The agent\\'s actions correspond to adjusting the different\n\"control knobs\" we can turn to modulate the microwave pulse. This\nsetting allows the agent to react to the qubit\\'s peculiarities and\nadapt the pulse parameters accordingly to execute the target gate.\n\nThe hardest part of this approach is extracting information about the\nqubit\\'s evolution under the pulse, since observing it destroys the\nquantum state. Here, we follow a similar approach to the one introduced\nin. The main idea is to split the pulse program into segments of\nconstant properties, commonly known as a piece-wise constant (PWC)\npulse, and evaluate the intermediate states between segments:\n\n1.  We fix the total duration and the number of segments of the PWC\n    pulse.\n2.  We reset the qubit.\n3.  We perform quantum tomography to determine the qubit\\'s state.\n4.  With this information, the agent fixes the parameters for the next\n    pulse segment.\n5.  We reset the qubit and execute the pulse up to the last segment with\n    fixed parameters.\n6.  We repeat steps 3-5 until we reach the end of the pulse and evaluate\n    the average gate fidelity.\n\nThe images below show a schematic representation of the main points in\nthe protocol. In the first image on the left, the first two pulse\nsegments (blue) are executed as a shorter pulse than the final result.\nThis is repeated several times to perform quantum state tomography of\nthe state, whose result is used to determine the parameters of the next\npulse segment (yellow). Afterwards, the system is reset, and the first\nthree segments are executed, which leads to the second figure on the\nright. There, the steps are repeated in the same order, evolving the\nqubit with the first three segments several times to characterize their\neffect and determine the parameters of the fourth pulse segment,\nreaching the end of the pulse. Given that all the pulse segments are now\nfixed, we proceed with the evaluation of the pulse with respect to the\ntarget gate that we wish to execute.\n\n![](https://blog-assets.cloud.pennylane.ai/demos/tutorial_rl_pulse/main/_assets/images/sketch_protocol.png)\n\nWith this protocol, the agent iteratively builds a PWC pulse that is\ntailored to the specifics of the qubit. Even though this protocol\ninvolves multiple (partial) executions of the pulse to perform the\nintermediate tomography steps, the overall cost remains low provided\nthat it is only for the qubit(s) involved in the gate, typically one or\ntwo.\n\n## Building a $R_X(\\pi/2)$ calibrator\n\nLet\\'s take all these concepts and apply them to train a reinforcement\nlearning agent to calibrate the single-qubit $R_X(\\pi/2)$ gate (a.k.a.\n$\\sqrt{X}$), which is a common native gate in superconducting quantum\ncomputers. To do so, we need to define:\n\n-   The environment (hardware, actions, and rewards)\n-   The agent (the policy and how to act)\n-   The learning algorithm\n\nThen, we\\'ll put all the pieces together and train our agent. Let\\'s\nintroduce these concepts one by one and implement them from scratch with\nPennyLane and JAX.\n\n### The environment\n\nAs we mentioned earlier, the environment contains all the information\nabout the problem. In an experimental setting, the actual quantum\ncomputer and how we interact with it would constitute the environment.\nIn this demo, we will simulate it with PennyLane.\n\nWe start by defining the quantum hardware. As we mentioned above, we\nwill simulate a superconducting quantum computer. The PennyLane\n[pulse](https://docs.pennylane.ai/en/stable/code/qml_pulse.html#module-$)\nmodule provides the tools to simulate quantum systems through time,\nallowing us to control quantum computers at the lowest pulse level. To\nperform the simulation, we will define an effective time-dependent\nHamiltonian for the hardware. We often distinguish between two main\ncomponents: a constant drift term that describes the interaction between\nthe qubits in our system (see\n[transmon_interaction](https://docs.pennylane.ai/en/stable/code/api/pennylane.pulse.transmon_interaction.html#$)),\nand a time-dependent drive term that accounts for the pulse (see\n[transmon_drive](https://docs.pennylane.ai/en/stable/code/api/pennylane.pulse.transmon_drive.html#$)).\nThe time-dependent Hamiltonian for a driven qubit is:\n\n$$H = \\underbrace{-\\frac{\\omega_q}{2}\\sigma^z}_{H_{int}} + \\underbrace{\\Omega(t)\\sin(\\phi(t) + \\omega_p t)\\sigma^y\\,}_{H_{drive}},$$\n\nwhere $\\omega_q,\\omega_p$ are the frequencies of the qubit and the\npulse, respectively, and $\\sigma^y,\\sigma^z$ denote the second and third\nPauli matrices.\n\nIn order to keep the implementation as simple as possible, we will work\nwith a single-qubit device. At the end of the demo, we provide insights\non how to extend the implementation to multi-qubit devices and gates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\n\n# Quantum computer\nqubit_freqs = [4.81]  # GHz\nconnections = []  # No connections\ncouplings = []  # No couplings\nwires = [0]\n\nH_int = qml.pulse.transmon_interaction(qubit_freqs, connections, couplings, wires)\n\n# Microwave pulse\npulse_duration = 22.4  # ns\nn_segments = 8\nsegment_duration = pulse_duration / n_segments\n\nfreq = qubit_freqs[0]  # Resonant with the qubit\namplitude = qml.pulse.pwc(pulse_duration)\nphase = qml.pulse.pwc(pulse_duration)\n\nH_drive = qml.pulse.transmon_drive(amplitude, phase, freq, wires)\n\n# Full time-dependent parametrized Hamiltonian\nH = H_int + H_drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have the effective model of our system, we need to simulate\nits time evolution, which can be easily done with\n[evolve](https://docs.pennylane.ai/en/stable/code/api/pennylane.evolve.html#$).\nSince we are simulating the whole process, we can speed up the process\nby simplifying the qubit reset, evolution and tomography steps, which\nare mostly intended for the execution on actual hardware. Here, we can\nsimply pause the time-evolution simulation, look at the qubit\\'s state,\nand then continue after the agent chooses the parameters of the\nsubsequent segment. Hence, the environment\\'s state will be exactly the\nqubit\\'s state.\n\nWe will do it with a\n[QNode](https://docs.pennylane.ai/en/stable/code/api/pennylane.QNode.html#$)\nthat can evolve several states in parallel following different pulse\nprograms to speed up the process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import jax\nfrom functools import partial\n\ndevice = qml.device(\"default.qubit\", wires=1)\n\n\n@partial(jax.jit, static_argnames=\"H\")\n@partial(jax.vmap, in_axes=(0, None, 0, None))\n@qml.qnode(device=device, interface=\"jax\")\ndef evolve_states(state, H, params, t):\n    qml.StatePrep(state, wires=wires)\n    qml.evolve(H)(params, t, atol=1e-5)\n    return qml.state()\n\n\nstate_size = 2 ** len(wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a model of the quantum computer and we have defined the\nenvironment\\'s states, we can proceed to define the actions. As we\nmentioned before, the actions will adjust the knobs we can turn to\ngenerate the microwave pulse. We have four parameters to play with in\nour pulse program: amplitude $\\Omega(t)$, phase $\\phi(t)$, frequency\n$\\omega_p,$ and duration. Out of those, we fix the duration beforehand\n(point 1 in the protocol), and we will always work with resonant pulses\nwith the qubit, thus fixing the frequency $\\omega_p=\\omega_q.$\n\nHence, we will let the agent change the amplitude and the phase for\nevery segment in our pulse program. To keep the pipeline as simple as\npossible, we will discretize their values within an\nexperimentally-feasible range, and associate every action to a\ncombination of amplitude and phase values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n\njax.config.update(\"jax_enable_x64\", True)  # Coment this line for a faster execution\n\nvalues_phase = jnp.linspace(-jnp.pi, jnp.pi, 9)[1:]  # 8 phase values\nvalues_ampl = jnp.linspace(0.0, 0.2, 11)  # 11 amplitude values\nctrl_values = jnp.stack(\n    (jnp.repeat(values_ampl, len(values_phase)), jnp.tile(values_phase, len(values_ampl))), axis=1\n)\nn_actions = len(ctrl_values)  # 8x11 = 88 possible actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we need to define the reward function. In the typical\nreinforcement learning setting, there are rewards after every action is\nperformed, as in the schematic depiction above. However, in some cases,\nthose rewards are zero until the task is finished. For example, if we\nare training an agent to play chess, we cannot evaluate every single\nmove on its own, and we need to wait until the game is resolved in order\nto provide the agent with a meaningful reward.\n\nOur case is similar to the chess example. The intermediate states\nvisited along the time evolution do not necessarily provide a clear\nindication of how well the target gate is being executed. It is only\nonce we have gone through all the pulse segments that we can see the\nfinal outcome and evaluate it as a whole, just like a chess strategy is\nevaluated based on the match\\'s result. The reward will be the average\ngate fidelity of our pulse program with respect to the target gate.\n\nIn order to evaluate it, we sample several random initial states and\napply the pulse program a few consecutive times. Then, we compute the\naverage fidelity between the resulting intermediate and final states\nfrom our pulse, and the expected states from the target gate. Finally,\nwe take the weighted average of all the fidelities, giving more\nrelevance to the initial gate applications. This process of repeatedly\napplying the pulse programs accumulates the errors, making our reward\nfunction more sensitive to them. This allows the agent to better refine\nthe pulse programs.\n\nThe number of initial states and gate repetitions are hyperparameters\nthat will add up really quickly with others such as the pulse duration,\nthe segment duration, and so on. In order to keep the code as clean as\npossible, we will put all of them in a `config` container (a\njit-friendly named tuple) and the code below will assume this container\nis being passed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target = jnp.array(qml.RX(jnp.pi / 2, 0).matrix())  # RX(pi/2) gate\n\n\n@partial(jax.jit, static_argnames=[\"H\", \"config\"])\n@partial(jax.vmap, in_axes=(0, None, None, None, None))\ndef compute_rewards(pulse_params, H, target, config, subkey):\n    \"\"\"Compute the reward for the pulse program based on the average gate fidelity.\"\"\"\n    n_gate_reps = config.n_gate_reps\n    # Sample the random initial states\n    states = jnp.zeros((config.n_eval_states, n_gate_reps + 1, state_size), dtype=complex)\n    states = states.at[:, 0, :].set(sample_random_states(subkey, config.n_eval_states, state_size))\n    target_states = states.copy()\n\n    # Repeatedly apply the gates and store the intermediate states\n    matrix = get_pulse_matrix(H, pulse_params, config.pulse_duration)\n    for s in range(n_gate_reps):\n        states = states.at[:, s + 1].set(apply_gate(matrix, states[:, s]))\n        target_states = target_states.at[:, s + 1].set(apply_gate(target, target_states[:, s]))\n\n    # Compute all the state fidelities (excluding the initial states)\n    overlaps = jnp.einsum(\"abc,abc->ab\", target_states[:, 1:], jnp.conj(states[:, 1:]))\n    fidelities = jnp.abs(overlaps) ** 2\n\n    # Compute the weighted average gate fidelities\n    weights = 2 * jnp.arange(n_gate_reps, 0, -1) / (n_gate_reps * (n_gate_reps + 1))\n    rewards = jnp.einsum(\"ab,b->a\", fidelities, weights)\n    return rewards.mean()\n\n\n@partial(jax.jit, static_argnames=[\"n_states\", \"dim\"])\ndef sample_random_states(subkey, n_states, dim):\n    \"\"\"Sample random states from the Haar measure.\"\"\"\n    subkey0, subkey1 = jax.random.split(subkey, 2)\n\n    s = jax.random.uniform(subkey0, (n_states, dim))\n    s = -jnp.log(jnp.where(s == 0, 1.0, s))\n    norm = jnp.sum(s, axis=-1, keepdims=True)\n    phases = jax.random.uniform(subkey1, s.shape) * 2.0 * jnp.pi\n    random_states = jnp.sqrt(s / norm) * jnp.exp(1j * phases)\n    return random_states\n\n\ndef get_pulse_matrix(H, params, time):\n    \"\"\"Compute the unitary matrix associated to the time evolution of H.\"\"\"\n    return qml.evolve(H)(params, time, atol=1e-5).matrix()\n\n\n@jax.jit\ndef apply_gate(matrix, states):\n    \"\"\"Apply the unitary matrix of the gate to a batch of states.\"\"\"\n    return jnp.einsum(\"ab,cb->ca\", matrix, states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The agent\n\nThe agent is a rather simple entity: it observes a state from the\nenvironment and selects an action among the ones we have defined above.\nThe action selection is performed according to a policy, which is\ntypically denoted by $\\pi.$ In this case, we will use a stochastic\npolicy $\\pi_{\\mathbf{\\theta}}(a_i|s_t)$ that provides the probability of\nchoosing the action $a_i$ given an observed state $s_t$ at a given time\n$t,$ according to some parameters $\\mathbf{\\theta}.$ Learning the\noptimal policy $\\pi^*$ will consist on learning the parameters\n$\\mathbf{\\theta}^*$ that best approximate it\n$\\pi_{\\mathbf{\\theta}^*}\\approx\\pi^*.$\n\nWe parametrize the policy with a small feed-forward neural network that\ntakes the state $s_t$ as input, and provides the probability to select\nevery action $a_i$ at the end. Therefore, the input and output layers\nhave `state_size=2` and `n_actions=88` neurons, respectively. We include\na hidden layer in between with a hyperbolic tangent activation function.\nLet\\'s implement this with\n[Flax](https://flax.readthedocs.io/en/latest/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from flax import linen as nn\n\n\n# Define the architecture\nclass MLP(nn.Module):\n    \"\"\"Multi layer perceptron (MLP) with a single hidden layer.\"\"\"\n\n    hidden_size: int\n    out_size: int\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Dense(self.hidden_size)(x)\n        x = nn.tanh(x)\n        x = nn.Dense(self.out_size)(x)\n        return nn.softmax(jnp.sqrt((x * x.conj()).real))\n\n\npolicy_model = MLP(hidden_size=30, out_size=n_actions)\n\n# Initialize the parameters passing a mock sample\nkey = jax.random.PRNGKey(3)\nkey, subkey = jax.random.split(key)\n\nmock_state = jnp.empty((1, state_size))\npolicy_params = policy_model.init(subkey, mock_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To act in the environment, we simply need to pass the state through the\nnetwork and sample an action according to the discrete probability\ndistribution provided by the output layer. However, we will see how to\nimplement it below to act and compute the policy gradient at once.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The learning algorithm\n\nAmong the plethora of reinforcement learning algorithms, we use the\nREINFORCE algorithm. It belongs to the policy gradient algorithms\nfamily, which proposes a parametrization of the policy (as we have\nconveniently done in the previous section) and directly optimizes its\nparameters. The main principle of REINFORCE is to directly modify the\npolicy to favour series of actions within the agent\\'s experience that\nhave led to high rewards. This way, past beneficial actions are more\nlikely to happen again.\n\nWe achieve this by maximizing the expected *return* of the policy. To\nkeep it simple, we can understand the return (typically $G$) as the\nweighted sum of rewards along an *episode*, which are full executions of\nour reinforcement learning \"game\". In our case, an episode would be the\nfull execution of a pulse program, which is comprised by several\ninteractions between the agent and the environment. Since the reward\nwill only be given at the end, we will take the return to be the final\nreward.\n\nWe perform the maximization of the expected return by gradient ascent\nover the policy parameters. We can compute the gradient of the expected\nreturn as follows\n\n$$\\nabla_{\\mathbf{\\theta}} \\mathbb{E}\\left[G\\right] = \\mathbb{E}\\left[\\sum_{t=0}^{T-1}G_t\\nabla_{\\mathbf{\\theta}}\\log\\pi_{\\mathbf{\\theta}}(a_t|s_t)\\right],$$\n\nwhere the expectation values are over episodes sampled following the\npolicy $\\pi_{\\mathbf{\\theta}}.$ The sum goes over the episode time steps\n$t,$ where the agent observes the state $s_t$ and chooses to perform the\naction $a_t.$ The term\n$\\nabla_{\\mathbf{\\theta}}\\log\\pi_{\\mathbf{\\theta}}(a_t|s_t)$ is known as\nthe *score function* and it is the gradient of the logarithm of the\nprobability with which the action is taken. Finally, $G_t$ is the return\nassociated to the episode from time $t$ onwards, which is always the\nfinal reward of the episode, as we mentioned. This expression allows us\nto compute the gradient of our policy parameters without explicitly\nmodeling the environment, hence the name \\\"model-free\\\" reinforcement\nlearning.\n\n> > Note\n>\n> At time $t,$ an action $a_t$ on state $s_t$[ leads to the next state\n> :math:\\`s\\_{t+1}]{.title-ref} and yields a reward $r_{t+1}.$ The final\n> action is taken at time $T-1$ which yields the final state $s_T$ and\n> reward $r_T.$ The return is the weighted sum of rewards obtained along\n> an episode:\n>\n> $$G=\\sum_{t=0}^{T-1} \\gamma^t r_{t+1}\\,$$\n>\n> where $\\gamma\\in[0, 1]$ is a *discount factor* that favours early\n> rewards vs latter ones. For instance, $\\gamma\\to0$ only values\n> immediate rewards, whereas $\\gamma\\to1$ accounts equally for all the\n> rewards regardless of the time. The return from time $t$ weights the\n> rewards relative to the given time:\n>\n> $$G_t=\\sum_{k=0}^{T-1-t} \\gamma^k r_{k+t+1},$$\n>\n> where $k$ denotes the number of steps after $t.$ Note that\n> $G\\equiv G_{t=0}$ by definition. The return can also be computed\n> recursively following the relationship\n> $G_t = r_{t+1} + \\gamma G_{t+1},$ a property exploited by some\n> reinforcement learning algorithms.\n>\n> In our case, we\\'re in the limit of $\\gamma=1,$ provided that we only\n> consider the final reward $r_T$ ($r_{t\\neq T}=0$) and we fix the total\n> number of interactions $T$ beforehand, given by the number of pulse\n> segments. This greatly simplifies the expressions and the return is\n> $G=G_t=r_T \\,\\forall t.$\n\nTo learn the optimal policy, we can estimate the gradient\n$\\nabla_{\\mathbf{\\theta}} \\mathbb{E}\\left[G\\right]$ by sampling a bunch\nof episodes following the current policy $\\pi_{\\mathbf{\\theta}}.$ Then,\nwe can perform a gradient ascent update to the policy parameters\n$\\mathbf{\\theta},$ sample some episodes with the new parameters, and\nrepeat until we converge to the optimal policy.\n\nLet\\'s implement these ideas one by one, starting by the episode\nsampling. The episodes start with the qubit in the $|0\\rangle$ state.\nThen, we start the observe-act loop, where the agent observes the\nqubit\\'s state and chooses the parameters for the next pulse segment\naccording to its policy. We can sample the action and compute the score\nfunction at the same time to make the code as efficient as possible.\nFinally, when we reach the end of the pulse program, we compute its\nreward. We like it when things go fast, so we\\'ll sample all the\nepisodes in parallel!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnames=[\"H\", \"config\"])\ndef play_episodes(policy_params, H, ctrl_values, target, config, key):\n    \"\"\"Play episodes in parallel.\"\"\"\n    n_episodes, n_segments = config.n_episodes, config.n_segments\n\n    # Initialize the qubits on the |0> state\n    states = jnp.zeros((n_episodes, n_segments + 1, target.shape[0]), dtype=complex)\n    states = states.at[:, 0, 0].set(1.0)\n\n    # Perform the PWC evolution of the pulse program\n    pulse_params = jnp.zeros((n_episodes, 2, n_segments))\n    actions = jnp.zeros((n_episodes, n_segments), dtype=int)\n    score_functions = []\n    for s in range(config.n_segments):\n        # Observe the current state and select the parameters for the next pulse segment\n        sf, (a, key) = act(states[:, s], policy_params, key)\n        pulse_params = pulse_params.at[..., s].set(ctrl_values[a])\n\n        # Evolve the states with the next pulse segment\n        time_window = (\n            s * config.segment_duration,  # Start time\n            (s + 1) * config.segment_duration,  # End time\n        )\n        states = states.at[:, s + 1].set(evolve_states(states[:, s], H, pulse_params, time_window))\n\n        # Save the experience for posterior learning\n        actions = actions.at[:, s].set(a)\n        score_functions.append(sf)\n\n    # Compute the final reward\n    key, subkey = jax.random.split(key)\n    rewards = compute_rewards(pulse_params, H, target, config, subkey)\n    return states, actions, score_functions, rewards, key\n\n\n@jax.jit\ndef act(states, params, key):\n    \"\"\"Act on states with the current policy params.\"\"\"\n    keys = jax.random.split(key, states.shape[0] + 1)\n    score_funs, actions = score_function_and_action(params, states, keys[1:])\n    return score_funs, (actions, keys[0])\n\n\n@jax.jit\n@partial(jax.vmap, in_axes=(None, 0, 0))\n@partial(jax.grad, argnums=0, has_aux=True)\ndef score_function_and_action(params, state, subkey):\n    \"\"\"Sample an action and compute the associated score function.\"\"\"\n    probs = policy_model.apply(params, state)\n    action = jax.random.choice(subkey, policy_model.out_size, p=probs)\n    return jnp.log(probs[action]), action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a way to sample the episodes, we need to process the\ncollected experience to compute the gradient of the expected return\n$\\nabla_{\\mathbf{\\theta}} \\mathbb{E}\\left[G\\right].$ First, however, we\nwill define two utility functions: one to add a list of pytrees together\n(helps with the temporal sum), and one to add extra dimensions to arrays\nto enable broadcasting. These solve a couple of technicalities that will\nmake the following code much more readable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@jax.jit\ndef sum_pytrees(pytrees):\n    \"\"\"Sum a list of pytrees.\"\"\"\n    return jax.tree_util.tree_map(lambda *x: sum(x), *pytrees)\n\n\n@jax.jit\ndef adapt_shape(array, reference):\n    \"\"\"Adapts the shape of an array to match the reference (either a batched vector or matrix).\n    Example:\n    >>> a = jnp.ones(3)\n    >>> b = jnp.ones((3, 2))\n    >>> adapt_shape(a, b).shape\n    (3, 1)\n    >>> adapt_shape(a, b) + b\n    Array([[2., 2.],\n           [2., 2.],\n           [2., 2.]], dtype=float32)\n    \"\"\"\n    n_dims = len(reference.shape)\n    if n_dims == 2:\n        return array.reshape(-1, 1)\n    return array.reshape(-1, 1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compute the gradient, we need to compute the sum within the\nexpectation value for every episode. However, we will give this\nexpression a twist and bring our reinforcement learning skills a step\nfurther. We will subtract a baseline $b$ to the return such that\n\n$$\\nabla_{\\mathbf{\\theta}} \\mathbb{E}\\left[G\\right] = \\mathbb{E}\\left[\\sum_{t=0}^{T-1}(G_t - b(s_t))\\nabla_{\\mathbf{\\theta}}\\log\\pi_{\\mathbf{\\theta}}(a_t|s_t)\\right].$$\n\nIntuitively, the magnitude of the reward is arbitrary and depends on our\nfunction of choice. Hence, it provides the same information if we shift\nit. For example, our reward based on the average gate fidelity is bound\nbetween zero and one, but everything would conceptually be the same if\nwe added one. In a sense, we could consider the original expression to\nhave a baseline of zero, and this new one to be a generalization. Any\nbaseline is valid provided that it does not depend on the action $a_t$\nbecause this ensures it has a null expectation value, leaving the\ngradient unaffected.\n\nWhile these baselines leave the expectation value of the gradient\nunchanged, they do have an impact in its variance. Here, we will\nimplement the optimal state-independent baseline that minimizes the\nvariance of the gradient. The optimal baseline for the $k$-th component\nof the gradient is:\n\n$$b_k = \\frac{\\mathbb{E}\\left[G_t\\left(\\partial_{\\theta_k}\\log\\pi_{\\mathbf{\\theta}}(a|s)\\right)^2\\right]}{\\mathbb{E}\\left[\\left(\\partial_{\\theta_k}\\log\\pi_{\\mathbf{\\theta}}(a|s)\\right)^2\\right]}\\,,$$\n\nwhere $\\partial_{\\theta_k}\\log\\pi_{\\mathbf{\\theta}}(a|s)$ is the $k$-th\ncomponent of the score function. Thus, this baseline has the same shape\nas the gradient. You can find a proof in Chapter 6.3.1 in. Reducing the\nvariance of our estimates significantly speeds up the training process\nby providing better gradient updates with less samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@jax.jit\ndef reinforce_gradient_with_baseline(episodes):\n    \"\"\"Estimates the parameter gradient from the episodes with a state-independent baseline.\"\"\"\n    _, _, score_functions, returns = episodes\n    ret_episodes = returns.sum()  # Sum of episode returns to normalize the final value\n    # b\n    baseline = compute_baseline(episodes)\n    # G - b\n    ret_minus_baseline = jax.tree_util.tree_map(lambda b: adapt_shape(returns, b) - b, baseline)\n    # sum((G - b) * sf)\n    sf_sum = sum_pytrees(\n        [jax.tree_util.tree_map(lambda r, s: r * s, ret_minus_baseline, sf) for sf in score_functions]\n    )\n    # E[sum((G - b) * sf)]\n    return jax.tree_util.tree_map(lambda x: x.sum(0) / ret_episodes, sf_sum)\n\n\n@jax.jit\ndef compute_baseline(episodes):\n    \"\"\"Computes the optimal state-independent baseline to minimize the gradient variance.\"\"\"\n    _, _, score_functions, returns = episodes\n    n_episodes = returns.shape[0]\n    n_segments = len(score_functions)\n    total_actions = n_episodes * n_segments\n    # Square of the score function: sf**2\n    sq_sfs = jax.tree_util.tree_map(lambda sf: sf**2, score_functions)\n    # Expected value: E[sf**2]\n    exp_sq_sfs = jax.tree_util.tree_map(\n        lambda sqsf: sqsf.sum(0, keepdims=True) / total_actions, sum_pytrees(sq_sfs)\n    )\n    # Return times score function squared: G*sf**2\n    r_sq_sf = sum_pytrees(\n        [jax.tree_util.tree_map(lambda sqsf: adapt_shape(returns, sqsf) * sqsf, sq_sf) for sq_sf in sq_sfs]\n    )\n    # Expected product: E[G_t*sf**2]\n    exp_r_sq_sf = jax.tree_util.tree_map(lambda rsqsf: rsqsf.sum(0, keepdims=True) / total_actions, r_sq_sf)\n    # Ratio of espectation values: E[G_t*sf**2] / E[sf**2]  (avoid dividing by zero)\n    return jax.tree_util.tree_map(lambda ersq, esq: ersq / jnp.where(esq, esq, 1.0), exp_r_sq_sf, exp_sq_sfs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can choose any optimizer that we like to perform the policy\nparameter updates with the gradient information. We will use the Adam\noptimizer provided by\n[Optax](https://optax.readthedocs.io/en/latest/api.html#optax.adam).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import optax\n\n\ndef get_optimizer(params, learning_rate):\n    \"\"\"Create and initialize an Adam optimizer for the parameters.\"\"\"\n    optimizer = optax.adam(learning_rate)\n    opt_state = optimizer.init(params)\n    return optimizer, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we will define our parameter update function to perform the gradient\nascent step. These optimizers default to gradient descent, which is the\nmost common in machine learning problems. Hence, we\\'ll have to subtract\nthe parameter update they provide to go in the opposite direction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def update_params(params, gradients, optimizer, opt_state):\n    \"\"\"Update model parameters with gradient ascent.\"\"\"\n    updates, opt_state = optimizer.update(gradients, opt_state, params)\n    new_params = jax.tree_util.tree_map(lambda p, u: p - u, params, updates)  # Negative update\n    return new_params, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The training\n\nWe have all the building blocks that we need to train an $R_X(\\pi/2)$\ncalibrator for our superconducting quantum computer. We just need to\nensemble the pieces together and choose the right parameters for the\ntask.\n\nLet\\'s define the config object that will contain all the\nhyperparameters of the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n\nhyperparams = [\n    \"pulse_duration\",  # Total pulse duration\n    \"segment_duration\",  # Duration of every pulse segment\n    \"n_segments\",  # Number of pulse segments\n    \"n_episodes\",  # Episodes to estimate the gradient\n    \"n_epochs\",  # Training iterations\n    \"n_eval_states\",  # Random states to evaluate the fidelity\n    \"n_gate_reps\",  # Gate repetitions for the evaluation\n    \"learning_rate\",  # Step size of the parameter update\n]\nConfig = namedtuple(\"Config\", hyperparams, defaults=[None] * len(hyperparams))\n\nconfig = Config(\n    pulse_duration=pulse_duration,\n    segment_duration=segment_duration,\n    n_segments=8,\n    n_episodes=200,\n    n_epochs=320,\n    n_eval_states=200,\n    n_gate_reps=2,\n    learning_rate=5e-3,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the training loop:\n\n1.  Sample episodes\n2.  Compute the gradient\n3.  Update the policy parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer, opt_state = get_optimizer(policy_params, config.learning_rate)\n\nlearning_rewards = []\nfor epoch in range(config.n_epochs):\n    *episodes, key = play_episodes(policy_params, H, ctrl_values, target, config, key)\n    grads = reinforce_gradient_with_baseline(episodes)\n    policy_params, opt_state = update_params(policy_params, grads, optimizer, opt_state)\n\n    learning_rewards.append(episodes[3].mean())\n    if (epoch % 40 == 0) or (epoch == config.n_epochs - 1):\n        print(f\"Iteration {epoch}: reward {learning_rewards[-1]:.4f}\")\n\nimport matplotlib.pyplot as plt\n\nplt.plot(learning_rewards)\nplt.xlabel(\"Training iteration\")\nplt.ylabel(\"Average reward\")\nplt.grid(alpha=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The algorithm has converged to a policy with a very high average\nreward!! Let\\'s see what this agent is capable of.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calibrating the qubits\n\nAfter the training, we have a pulse calibrator for the $R_X(\\pi/2)$ gate\nwith a high average gate fidelity. The next step is to actually use it\nto calibrate the qubits in our device.\n\nNotice that, during the whole training process, the actions of our agent\nhave been stochastic. At every step, the policy provides the probability\nto choose every action and we sample one accordingly. When we deploy our\ncalibrator, we want it to consistently provide good pulse programs on a\nsingle pass through the qubit. Therefore, rather than sampling the\nactions, we take the one with the highest probability.\n\nLet\\'s define a function to extract the pulse program from the policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_pulse_program(policy_params, H, ctrl_values, config):\n    \"\"\"Extract the pulse program from the trained policy.\"\"\"\n    state = jnp.zeros((1, state_size), dtype=complex).at[:, 0].set(1.0)\n    pulse_params = jnp.zeros((1, ctrl_values.shape[-1], config.n_segments))\n    for s in range(config.n_segments):\n        probs = policy_model.apply(policy_params, state)\n        action = probs.argmax()\n        pulse_params = pulse_params.at[..., s].set(ctrl_values[action])\n        time_window = (s * config.segment_duration, (s + 1) * config.segment_duration)\n        state = evolve_states(state, H, pulse_params, time_window)\n    return pulse_params[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can evaluate the resulting average gate fidelity over several random\ninitial states. Remember that the reward is a combination of the gate\nfidelity over several gate repetitions, which is a lower bound to the\nactual gate fidelity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_program(pulse_program, H, target, config, subkey):\n    \"\"\"Compute the average gate fidelity over 1000 random initial states.\"\"\"\n    states = sample_random_states(subkey, 1000, state_size)\n    target_states = jnp.einsum(\"ab,cb->ca\", target, states)\n    pulse_matrix = qml.matrix(qml.evolve(H)(pulse_program, config.pulse_duration))\n    final_states = jnp.einsum(\"ab,cb->ca\", pulse_matrix, states)\n    fidelities = qml.math.fidelity_statevector(final_states, target_states)\n    return fidelities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s extract the pulse program for our qubit, the rotation axis of the\nresulting gate, and the average gate fidelity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pulse_program = get_pulse_program(policy_params, H, ctrl_values, config)\n\n\ndef vector_to_bloch(vector):\n    \"\"\"Transform a vector into Bloch sphere coordinates.\"\"\"\n    rho = jnp.outer(vector, vector.conj())\n    X, Y, Z = qml.PauliX(0).matrix(), qml.PauliY(0).matrix(), qml.PauliZ(0).matrix()\n    x, y, z = (\n        jnp.trace(rho @ X).real.item(),\n        jnp.trace(rho @ Y).real.item(),\n        jnp.trace(rho @ Z).real.item(),\n    )\n    return [x, y, z]\n\n\nmatrix = get_pulse_matrix(H, pulse_program, config.pulse_duration)\n_, evecs = jnp.linalg.eigh(matrix)\nrot_axis = vector_to_bloch(evecs[:, 0])\n\nfidelities = evaluate_program(pulse_program, H, target, config, key)\navg_gate_fidelity = fidelities.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the amplitude and phase over time to get a better idea of\nwhat\\'s going on. Furthermore, we can visualize the rotation axis in the\nBloch sphere to see its alignment with the $X$ axis. Despite the\ntraining reward being 0.982, the actual average gate fidelity is 0.993,\nshowing how it accumulates the errors to make the agent more sensitive\nand reach better results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import qutip\n\n\ndef plot_rotation_axes(rotation_axes, color=[\"#70CEFF\"], fig=None, ax=None):\n    \"\"\"Plot the rotation axes in the Bloch sphere.\"\"\"\n    bloch = qutip.Bloch(fig=fig, axes=ax)\n    bloch.sphere_alpha = 0.05\n    bloch.vector_color = color\n    bloch.add_vectors(rotation_axes)\n    bloch.render()\n\n\nts = jnp.linspace(0, pulse_duration - 1e-3, 100)\nfig, axs = plt.subplots(ncols=3, figsize=(14, 4), constrained_layout=True)\n\naxs[0].plot(ts, qml.pulse.pwc(pulse_duration)(pulse_program[0], ts), color=\"#70CEFF\", linewidth=3)\naxs[0].set_ylabel(\"Amplitude (GHz)\", fontsize=14)\naxs[0].set_yticks(values_ampl)\naxs[0].set_ylim([values_ampl[0], values_ampl[-1]])\n\naxs[1].plot(ts, qml.pulse.pwc(pulse_duration)(pulse_program[1], ts), color=\"#FFE096\", linewidth=3)\naxs[1].set_ylabel(\"Phase (rad)\", fontsize=14)\naxs[1].set_yticks(\n    values_phase,\n    [\"$-3\\pi/4$\", \"$-\\pi/2$\", \"$-\\pi/4$\", \"0\", \"$\\pi/4$\", \"$\\pi/2$\", \"$3\\pi/4$\", \"$\\pi$\"],\n)\naxs[1].set_ylim([values_phase[0] - 0.1, values_phase[-1] + 0.1])\n\nfor ax in axs[:2]:\n    ax.grid(alpha=0.3)\n    ax.tick_params(labelsize=12)\n    ax.set_xlabel(\"Time (ns)\", fontsize=14)\n\naxs[2].axis(\"off\")\nax2 = fig.add_subplot(1, 3, 3, projection=\"3d\")\nplot_rotation_axes(rot_axis, fig=fig, ax=ax2)\nax2.set_title(f\"Average gate fidelity {avg_gate_fidelity:.3f}\", fontsize=14)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Beyond single-qubit quantum computers and gates\n\nAll the concepts that we have learned throughout the demo are directly\napplicable to quantum computers with more qubits, two-qubit gates, and\neven other platforms beyond superconducting circuits. Perhaps the most\nstraightforward extension of the code presented here would be\ncalibrating an entangling gate, such as a CNOT. We will provide an\noverview of how to adapt this demo to do it, although they take\nsignificantly longer to train (mainly because the pulses need to be much\nlonger).\n\nThe first thing we need to build an entangling gate is a second qubit in\nour device with a different frequency coupled to the first one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Quantum computer\nqubit_freqs = [4.81, 4.88]  # GHz\nconnections = [[0, 1]]\ncouplings = [0.02]\nwires = [0, 1]\n\nH_int = qml.pulse.transmon_interaction(qubit_freqs, connections, couplings, wires)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The CNOT gate is typically constituted by a series of single-qubit\npulses and cross-resonant (CR) pulses. Every single-qubit pulse is\nresonant with the qubit it targets, i.e., it has the same frequency,\nwhereas CR pulses are shone on the control qubit at the target\\'s\nfrequency. This way, the target is indirectly driven through the\ncoupling with the control qubit, which entangles them.\n\nWe will make the CR pulse take the first qubit as control and the second\nas target. We will implement what\\'s known as an echoed CR pulse, which\nconsists of flipping the state of the control qubit (applying an $X$\ngate) in the middle of the CR pulse. The second half of the pulse is the\nnegative of the previous one. This \"echoes out\" the rapid interactions\nbetween the qubits, while preserving the entangling slower interaction\nthat we are interested in, as introduced in.\n\nOur full pulse ansatz will consist of sandwiching the echoed CR gate\nbetween single qubit pulses. Therefore, we will have a total of six PWC\npulses: two single qubit ones, two CR, and two single qubit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Microwave pulse\npulse_duration_sq = 22.4  # Single qubit pulse duration\npulse_duration_cr = 100.2  # CR pulse duration\n\n\ndef get_drive(timespan, freq, wire):\n    \"\"\"Parametrized Hamiltonian driving the qubit in wire with a fixed frequency.\"\"\"\n    amplitude = qml.pulse.pwc(timespan)\n    phase = qml.pulse.pwc(timespan)\n    return qml.pulse.transmon_drive(amplitude, phase, freq, wire)\n\n\npulse_durations = jnp.array(\n    [\n        0,\n        pulse_duration_sq,\n        pulse_duration_sq,\n        pulse_duration_cr,\n        pulse_duration_cr,\n        pulse_duration_sq,\n        pulse_duration_sq,\n    ]\n)\nchange_times = jnp.cumsum(pulse_durations)\ntimespans = [(t0.item(), t1.item()) for t0, t1 in zip(change_times[:-1], change_times[1:])]\n\nH_sq_0_ini = get_drive(timespans[0], qubit_freqs[0], wires[0])\nH_sq_1_ini = get_drive(timespans[1], qubit_freqs[1], wires[1])\nH_cr_pos = get_drive(timespans[2], qubit_freqs[1], wires[0])  # Target qubit 0 with freq from 1\nH_cr_neg = get_drive(timespans[3], qubit_freqs[1], wires[0])\nH_sq_0_end = get_drive(timespans[4], qubit_freqs[0], wires[0])\nH_sq_1_end = get_drive(timespans[5], qubit_freqs[1], wires[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that the CR pulses are an order of magnitude longer than the\nsingle-qubit ones. This is because the drive on the target qubit is\ndampened by the coupling constant, thus requiring much longer times to\nobserve a comparable effect compared to driving it directly.\n\nWe now have to put everything together in the\n[QNode](https://docs.pennylane.ai/en/stable/code/api/pennylane.QNode.html#$)\nthat will be in charge of the evolution. We need to account for the\ncontrol qubit flip between CR pulses and revert it at the end, as well\nas to fix the second CR pulse to be the negative of the previous one.\nNotice that both CR pulses will share their parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "H_sq_ini = H_sq_0_ini + H_sq_1_ini\nH_sq_end = H_sq_0_end + H_sq_1_end\n\n\n@jax.jit\n@partial(jax.vmap, in_axes=(0, None, 0, None))\n@qml.qnode(device=device, interface=\"jax\")\ndef evolve_states(state, params, t):\n    params_sq, params_cr = params\n    qml.StatePrep(state, wires=wires)\n    # Single qubit pulses\n    qml.evolve(H_int + H_sq_ini)(params_sq, t, atol=1e-5)\n\n    # Echoed CR\n    qml.evolve(H_int + H_cr_pos)(params_cr, t, atol=1e-5)\n    qml.PauliX(0)  # Flip control qubit\n    qml.evolve(H_int - H_cr_neg)(params_cr, t, atol=1e-5)  # Negative CR\n    qml.PauliX(0)  # Recover control qubit\n\n    # Single qubit pulses\n    qml.evolve(H_int + H_sq_end)(params_sq, t, atol=1e-5)\n\n    return qml.state()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The state of the environment is now a 2-qubit state for which on the\nquantum computer we need to perform $\\mathcal{O}(4^2)$ measurements for\ntomography. We would need to decide how many segments we wish to split\neach pulse into, and define the appropriate `time_window` within\n`play_episodes`. This can be achieved by modifying the\n`config.segment_duration` to be an array that contains the time spans of\nevery segment, such that `time_window = config.segment_duration[s]`, or\nsimilar. Given that the negative CR pulse uses the same parameters as\nthe positive CR one, we can skip it as an entire segment merged with the\nlast from the positive one that does not involve any intermediate\ntomography steps.\n\nFinally, when dealing with quantum computers with several qubits, we can\nopt for two strategies: train specialized calibrators for every qubit\n(or qubit pair), or train a single general calibrator for all the\nqubits. In these cases, we need to define a separate drive Hamiltonian\nfor each individual qubit. Training individual specialized agents can be\ndone in parallel following the same principles introduced in this demo,\nwhich will make them robust to the various sources of noise. Training a\ngeneral agent is a bit more involved to adapt. Mainly, every episode\ncontrols the evolution of a randomly selected qubit with its own\n`H_drive`. This involves modifying `play_episodes` to sample the\nselected qubits, and carry their evolution under their respective\nHamiltonians. Notice that `evolve_states` should be parallelized over\nthe Hamiltonian too.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions\n\nIn this demo, we have learned how to design an experimentally-friendly\ncalibrator with reinforcement learning. To do so, we have learned the\nfundamental principles of reinforcement learning, the REINFORCE\nalgorithm, and how to frame the calibration of quantum computers within\nthis framework. Then, we have put everything together to calibrate a\nsingle-qubit gate, and we have learned how to apply the principles\nexplored here to gates involving multiple qubits and larger devices.\n\nThe method presented in this demo has several strengths. First of all,\nit does not require any model or prior information about the quantum\ncomputer, making it widely applicable across different quantum computing\nplatforms, even though we have only showed an application on a\nsuperconducting quantum computer. Furthermore, once we have invested\nresources in training the reinforcement learning calibrator, we can use\nit to recalibrate the qubits multiple times at a very low cost. In[^1],\nthe authors report an average gate fidelity of 0.995 on a 2-qubit gate,\nand 0.993 after 25 days of training!\n\nTo continue learning about this topic, try implementing one of the two\nextensions we explain above. To learn more about reinforcement learning,\nwe recommend[^2] for an introduction to the topic, and[^3] for an\nintroduction of machine learning for physics (reinforcement learning in\nchapter 6). To learn more about how superconducting quantum computers\nwork, see[^4] for an extensive review, and the related [PennyLane\ndocumentation](https://docs.pennylane.ai/en/stable/code/qml_pulse.html).\n\nFinally, check out the related demos for alternative ways to tune pulse\nprograms. In particular, [this\ndemo](https://pennylane.ai/qml/demos/tutorial_optimal_control/) for an\noptimal control approach to gate calibration, [this demo on optimizing\npulses using hardware compatible\ngradients](https://pennylane.ai/qml/demos/tutorial_odegen/), and [this\nmore general intro to differentiable pulse\nprogramming](https://pennylane.ai/qml/demos/tutorial_pulse_programming101/).\n\n[^1]: Y. Baum, et. al. (2019) \\\"Experimental Deep Reinforcement Learning\n    for Error-Robust Gate-Set Design on a Superconducting Quantum\n    Computer.\\\" [PRX Quantum 2(4),\n    040324](https://link.aps.org/doi/10.1103/PRXQuantum.2.040324).\n\n[^2]: R. S. Sutton and A. G. Barto. (2018) \\\"Reinforcement learning: An\n    introduction.\\\" [MIT\n    Press](https://mitpress.mit.edu/9780262039246/reinforcement-learning/).\n\n[^3]: A. Dawid, et. al. (2022) \\\"Modern applications of machine learning\n    in quantum sciences.\\\"\n    [arXiv:2204.04198](https://arxiv.org/abs/2204.04198).\n\n[^4]: P. Krantz, M. Kjaergaard, F. Yan, T. P. Orlando, S. Gustavsson and\n    W. D. Oliver. (2019) \\\"A quantum engineer\\'s guide to\n    superconducting qubits.\\\" [Applied physics reviews\n    6(2)](https://pubs.aip.org/aip/apr/article/6/2/021318/570326/A-quantum-engineer-s-guide-to-superconducting).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}